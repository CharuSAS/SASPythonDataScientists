{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAS Data Science Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access - Identify analysis tables that will be used in load those tables.<br> \n",
    "Explore/Investigate - Inspect tables to determine whether any changes are needed for data items due to data inconsistencies or data quality issues, as well as identify any new data items that need to be calculated. <br>\n",
    "<b>Prepare - Correct any data quality issues and create any new calculated items needed for analysis. </b><br>\n",
    "Analyze - Explore  data to identify any patterns, relationships, and trends. <br>\n",
    "Report - Develop interactive reports that can be shared via the web or a mobile device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1=pd.read_csv('/workspaces/myfolder/SASPythonDataScientists/pattern_decline_N_American_Bumblebees.csv', encoding='latin-1')\n",
    "df2=pd.read_csv('/workspaces/myfolder/SASPythonDataScientists/pattern_decline_Mexican_Bumblebees.csv' , encoding='latin-1')\n",
    "df3=pd.read_csv('/workspaces/myfolder/SASPythonDataScientists/Bumblebee_Others_Scientific_Common_Names.csv' , encoding='latin-1')\n",
    "df4=pd.read_csv('/workspaces/myfolder/SASPythonDataScientists/native_vs_nonnative_bumblebee_sighting_pollinators_of_farm_data_for_publication.csv' , encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate 2 data frames to combine North American(excluding Alaska) and Mexican Bumblebees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a quick look at the dimensions of the 2 dataframes we are about to concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# North American bumblebee decline dataframe\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mexican bumblebee decline dataframe\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation is a way to stitch dataframes along an axis, either row axis or column axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use concat() and pass it a list of DataFrames that you want to concatenate. Code for this task below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfconc=pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66931, 26)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfconc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenating along columns-concat() call like you did above, but you’ll also need to pass the axis parameter with a value of 1 or \"columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfconccol=pd.concat([df1,df2], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfconccol=pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfconccol.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter for north american bumblebees minus alaska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id institutionCode collectionCode      basisOfRecord  occurrenceID  \\\n",
      "0       1        USDA-ARS           BBSL  PreservedSpecimen   699384987.0   \n",
      "1       2        USDA-ARS           BBSL  PreservedSpecimen   699384988.0   \n",
      "2       3        USDA-ARS           BBSL  PreservedSpecimen   699384989.0   \n",
      "3       4        USDA-ARS           BBSL  PreservedSpecimen   699384990.0   \n",
      "4       5        USDA-ARS           BBSL  PreservedSpecimen   699384991.0   \n",
      "..    ...             ...            ...                ...           ...   \n",
      "19  66927        USDA-ARS           BBSL  PreservedSpecimen           NaN   \n",
      "20  66928        USDA-ARS           BBSL  PreservedSpecimen           NaN   \n",
      "21  66929        USDA-ARS           BBSL  PreservedSpecimen           NaN   \n",
      "22  66930        USDA-ARS           BBSL  PreservedSpecimen           NaN   \n",
      "23  66931        USDA-ARS           BBSL  PreservedSpecimen           NaN   \n",
      "\n",
      "   catalogNumber        recordedBy    year  month   day  ...  \\\n",
      "0     BBSL221088       W. Apperson  1970.0    7.0  27.0  ...   \n",
      "1     BBSL241571       W. Apperson  1970.0    7.0  27.0  ...   \n",
      "2          76122         B. Hevron  1989.0    6.0  16.0  ...   \n",
      "3       JPS30053  P.S. Bartholomew  1970.0    9.0  15.0  ...   \n",
      "4     BBSL226571       W.J. Hanson  1961.0    8.0  15.0  ...   \n",
      "..           ...               ...     ...    ...   ...  ...   \n",
      "19    BOMBUS1219               NaN  1928.0    8.0  19.0  ...   \n",
      "20    BOMBUS1348               NaN  1928.0    7.0  13.0  ...   \n",
      "21   BOMBUS33485               NaN  1930.0    9.0  13.0  ...   \n",
      "22   BOMBUS33755               NaN  1944.0    1.0   8.0  ...   \n",
      "23   BOMBUS37213               NaN  1933.0    6.0  19.0  ...   \n",
      "\n",
      "          identifiedBy        scientificName   kingdom      phylum    class  \\\n",
      "0                  NaN   Bombus occidentalis  Animalia  Arthropoda  Insecta   \n",
      "1                  NaN   Bombus occidentalis  Animalia  Arthropoda  Insecta   \n",
      "2   T.L. Griswold 1994       Bombus bifarius  Animalia  Arthropoda  Insecta   \n",
      "3   R.S. Jacobson 1991   Bombus occidentalis  Animalia  Arthropoda  Insecta   \n",
      "4                  NaN       Bombus bifarius  Animalia  Arthropoda  Insecta   \n",
      "..                 ...                   ...       ...         ...      ...   \n",
      "19                 NaN  Bombus pensylvanicus  Animalia  Arthropoda  Insecta   \n",
      "20                 NaN  Bombus pensylvanicus  Animalia  Arthropoda  Insecta   \n",
      "21                 NaN  Bombus pensylvanicus  Animalia  Arthropoda  Insecta   \n",
      "22                 NaN  Bombus pensylvanicus  Animalia  Arthropoda  Insecta   \n",
      "23                 NaN      Bombus impatiens  Animalia  Arthropoda  Insecta   \n",
      "\n",
      "          order  family   genus specificEpithet scientificNameAuthorship  \n",
      "0   Hymenoptera  Apidae  Bombus    occidentalis              Greene 1858  \n",
      "1   Hymenoptera  Apidae  Bombus    occidentalis              Greene 1858  \n",
      "2   Hymenoptera  Apidae  Bombus        bifarius             Cresson 1878  \n",
      "3   Hymenoptera  Apidae  Bombus    occidentalis              Greene 1858  \n",
      "4   Hymenoptera  Apidae  Bombus        bifarius             Cresson 1878  \n",
      "..          ...     ...     ...             ...                      ...  \n",
      "19  Hymenoptera  Apidae  Bombus   pensylvanicus            (DeGeer 1773)  \n",
      "20  Hymenoptera  Apidae  Bombus   pensylvanicus            (DeGeer 1773)  \n",
      "21  Hymenoptera  Apidae  Bombus   pensylvanicus            (DeGeer 1773)  \n",
      "22  Hymenoptera  Apidae  Bombus   pensylvanicus            (DeGeer 1773)  \n",
      "23  Hymenoptera  Apidae  Bombus       impatiens             Cresson 1863  \n",
      "\n",
      "[66931 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "dfnoal=dfconc[dfconc['stateProvince'] !='Alaska' ]\n",
    "print(dfnoal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnoal.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Frequency counts for scientific name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value_counts function returns the distinct values in a column along with their number of occurrences. Missing values are ignored by default. If we know that missing values exist in a column, it is best to count them as well. The dropna parameter is set to False to include the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting missing values is an essential step in data cleaning and preprocessing, but why do we see no missing values for scientificname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dfnoal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notna() function returns a Boolean Series where True represents a non-missing value and False represents a missing value. The sum() function is then used to count the number of True values, which represent the non-missing values.\n",
    "We then use the isna() method to create a Boolean mask of the DataFrame, where True indicates a missing value. We use the sum() method  to count the number of True values in each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfnoal['genus'].notna().sum())\n",
    "print(dfnoal['genus'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Filtering Rows Based on a Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering rows based on a condition is a common data management task that allows you to focus on a specific subset of your data. By applying a condition to a column, such as selecting rows where the pollinator_genus is Bumblebee, you can isolate and analyze the data that meets your criteria. This helps in drawing insights and making data-driven decisions based on relevant data subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id institutionCode collectionCode      basisOfRecord  occurrenceID  \\\n",
      "0       1        USDA-ARS           BBSL  PreservedSpecimen   699384987.0   \n",
      "1       2        USDA-ARS           BBSL  PreservedSpecimen   699384988.0   \n",
      "2       3        USDA-ARS           BBSL  PreservedSpecimen   699384989.0   \n",
      "3       4        USDA-ARS           BBSL  PreservedSpecimen   699384990.0   \n",
      "4       5        USDA-ARS           BBSL  PreservedSpecimen   699384991.0   \n",
      "..    ...             ...            ...                ...           ...   \n",
      "19  66927        USDA-ARS           BBSL  PreservedSpecimen           NaN   \n",
      "20  66928        USDA-ARS           BBSL  PreservedSpecimen           NaN   \n",
      "21  66929        USDA-ARS           BBSL  PreservedSpecimen           NaN   \n",
      "22  66930        USDA-ARS           BBSL  PreservedSpecimen           NaN   \n",
      "23  66931        USDA-ARS           BBSL  PreservedSpecimen           NaN   \n",
      "\n",
      "   catalogNumber        recordedBy    year  month   day  ...  \\\n",
      "0     BBSL221088       W. Apperson  1970.0    7.0  27.0  ...   \n",
      "1     BBSL241571       W. Apperson  1970.0    7.0  27.0  ...   \n",
      "2          76122         B. Hevron  1989.0    6.0  16.0  ...   \n",
      "3       JPS30053  P.S. Bartholomew  1970.0    9.0  15.0  ...   \n",
      "4     BBSL226571       W.J. Hanson  1961.0    8.0  15.0  ...   \n",
      "..           ...               ...     ...    ...   ...  ...   \n",
      "19    BOMBUS1219               NaN  1928.0    8.0  19.0  ...   \n",
      "20    BOMBUS1348               NaN  1928.0    7.0  13.0  ...   \n",
      "21   BOMBUS33485               NaN  1930.0    9.0  13.0  ...   \n",
      "22   BOMBUS33755               NaN  1944.0    1.0   8.0  ...   \n",
      "23   BOMBUS37213               NaN  1933.0    6.0  19.0  ...   \n",
      "\n",
      "          identifiedBy        scientificName   kingdom      phylum    class  \\\n",
      "0                  NaN   Bombus occidentalis  Animalia  Arthropoda  Insecta   \n",
      "1                  NaN   Bombus occidentalis  Animalia  Arthropoda  Insecta   \n",
      "2   T.L. Griswold 1994       Bombus bifarius  Animalia  Arthropoda  Insecta   \n",
      "3   R.S. Jacobson 1991   Bombus occidentalis  Animalia  Arthropoda  Insecta   \n",
      "4                  NaN       Bombus bifarius  Animalia  Arthropoda  Insecta   \n",
      "..                 ...                   ...       ...         ...      ...   \n",
      "19                 NaN  Bombus pensylvanicus  Animalia  Arthropoda  Insecta   \n",
      "20                 NaN  Bombus pensylvanicus  Animalia  Arthropoda  Insecta   \n",
      "21                 NaN  Bombus pensylvanicus  Animalia  Arthropoda  Insecta   \n",
      "22                 NaN  Bombus pensylvanicus  Animalia  Arthropoda  Insecta   \n",
      "23                 NaN      Bombus impatiens  Animalia  Arthropoda  Insecta   \n",
      "\n",
      "          order  family   genus specificEpithet scientificNameAuthorship  \n",
      "0   Hymenoptera  Apidae  Bombus    occidentalis              Greene 1858  \n",
      "1   Hymenoptera  Apidae  Bombus    occidentalis              Greene 1858  \n",
      "2   Hymenoptera  Apidae  Bombus        bifarius             Cresson 1878  \n",
      "3   Hymenoptera  Apidae  Bombus    occidentalis              Greene 1858  \n",
      "4   Hymenoptera  Apidae  Bombus        bifarius             Cresson 1878  \n",
      "..          ...     ...     ...             ...                      ...  \n",
      "19  Hymenoptera  Apidae  Bombus   pensylvanicus            (DeGeer 1773)  \n",
      "20  Hymenoptera  Apidae  Bombus   pensylvanicus            (DeGeer 1773)  \n",
      "21  Hymenoptera  Apidae  Bombus   pensylvanicus            (DeGeer 1773)  \n",
      "22  Hymenoptera  Apidae  Bombus   pensylvanicus            (DeGeer 1773)  \n",
      "23  Hymenoptera  Apidae  Bombus       impatiens             Cresson 1863  \n",
      "\n",
      "[66931 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where \n",
    "filtered_df = dfnoal.query('genus == \"Bombus\"')\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnoal[dfnoal['genus'] == 'Bombus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66931"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dfnoal['genus'] == 'Bombus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3740"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(~(df4['Species'] == 'Bombus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((df4['Species'] != 'Bombus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're diving into the world of bumblebees by buzzing through some data magic in Python! Imagine we've got one table that's packed with the common names of our favorite fuzzy pollinators, and another that's got their nesting habits. By merging the common names  with the nesting habits names into one tidy table, we're basically creating the ultimate bee database—bringing together the familiar and the formal. It's like giving each bee its proper name tag at the hive party! This way, we can easily connect the dots between the Latin and the layman's terms, making our bumblebee data analysis as sweet as honey. 🐝💻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a quick look at the dimensions of the tables we are about to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with our two bumblebee tables—one buzzing with scientific names and the other humming with common names—Python's merge() function is like a matchmaker for your data. The great thing about merge() is that it lets you decide exactly how these two tables come together. Say you want to merge them based on the ScientificName column, ensuring that each bee's formal identity pairs up perfectly with its everyday nickname. By using the on parameter, you can create the ultimate bee directory where the Latin meets the common, all while keeping your data as sharp as a bee's stinger! 🐝🔗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qIn the world of pandas, DataFrames have a merge() method,  with similar functionality to SAS joins. No need to sort ahead of time—perform all kinds of different joins by simply using the how keyword. It’s like a hive of possibilities for your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_merged = pd.merge(df3, df4, on=[\"SPECIES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_join = df1.merge(df3, on=[\"SCIENTIFICNAME\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " column names for dataframes are case sensitive.\n",
    "\n",
    "Dataframe column names are essentially string values, which are case sensitive in Python. Because of this, you will need to be careful whenever you utilize column names, such as when renaming a column, accessing columns or performing functions on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns = df3.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.columns = df4.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner = df3.merge(df4, on=[\"species\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner[['scientificname','commonname', 'nesting']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge df1 and df3 on column scientificname to see where bombus nests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df3, df4, on=[\"species\"])\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get me the all the Bombus species with nesting values -start here & clean up story around merging & concatenating\n",
    "merged[['Species'] == 'Bombus','nesting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the native vs non native bumblebee CSV file into a DataFrame for easy data manipulation and analysis.\n",
    "df5=pd.read_csv('/workspaces/myfolder/SASPythonDataScientists/native_vs_nonnative_bumblebee_sighting_pollinators_of_farm_data_for_publication.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.describe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Aggregating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keep columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbig = dfbig[['scientificName','stateProvince']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/47320572/pandas-groupby-and-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbig.groupby(['stateProvince','scientificName'])['scientificName'].count().head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chaining, create an obj with syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbig.value_counts(subset=['scientificName', 'stateProvince']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbig.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dfbig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get to know data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfbig.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you import data into a Pandas DataFrame, Pandas by default tries to know the data types of each column. Columns with text are by default marked as Object datatype.\n",
    "\n",
    "But Object dtype have a much broader scope. They can not only include strings, but also any other data that Pandas doesn't understand.\n",
    "\n",
    "After Pandas 1.0 (now 1.1.2), there's a dedicated dtype to handle and work with text data, that is, String.🤔\n",
    "\n",
    "How is this important?\n",
    "\n",
    "When a column is Object type, it does not necessarily mean that all the values will be string.\n",
    "\n",
    "In fact, they can all be numbers, or a mixture of string, integers and floats.\n",
    "\n",
    "With this discrepancy present, you can not do any string operation on the column straightaway.\n",
    "\n",
    "Moreover, having dtype as Object will make it less clear to work with just text and exclude the non-text values.\n",
    "\n",
    "With the new String dtype, the values are explicitly treated as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean(), inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the DataFrame to use best possible dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfconv = dfbig.convert_dtypes()\n",
    "dfconv.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfconv.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Python is a power language, it may not have ready-to-use functions that replicates SAS procedure exactly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, first group by age_category gender, aggregate by statistical functions, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=df1.groupby(['year','month']).agg(['mean','max','min','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=df1.groupby(['year','month']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request first 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfconv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfconv.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy vs lazy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all instances of bee names ending in 'en' 'ed' or with a '-' using perl regular expression\n",
    "\n",
    " code to repeat this perl code where prxmatch('/[a-z]ern|ed|-( |^(1))/', commonname);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[a-z]ern|ed|-( |^(1))'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values Fill missing values with the column mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values is crucial for maintaining data integrity and ensuring accurate analysis. Missing data can be filled using various methods, such as replacing them with the column mean. This task involves using the fillna function to fill any NaN (Not a Number) values in the DataFrame with the mean of their respective columns, thereby preventing potential biases or errors in subsequent analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Aggregating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a Function to Each Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a function to each column allows you to perform element-wise operations across the DataFrame. This task involves using the apply function with a lambda function to modify the values in each column. For example, multiplying each element by 2. This technique is useful for standardizing data, performing calculations, and transforming data values as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a lambda function to each column\n",
    "df = df.apply(lambda x: x*2)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting DataFrame to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('cleaned_data.csv', index=False)\n",
    "print(\"DataFrame exported to cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=df1['column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ari 20aug 24 frequency counts for all categorical variables\n",
    "for loop-instead of looping over index; iterating thro categorical columns\n",
    "column =goes thro each of the list elements\n",
    "display - print -running crosstab method in pandas;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables=[]\n",
    "for column in df1.select_dtypes(include=['object']).columns:\n",
    "    tables.append(pd.crosstab(index=df1[column], columns='number of observations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df1.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=df1[column], columns='number of observations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df1.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=df1[column], columns='% observations', normalize='columns')*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcount=df1.groupby(list(df1)).size()\n",
    "print(\"counts\",dfcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcount=df1.groupby(['country']).size()\n",
    "print(\"counts\",dfcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcount=dfconv.groupby(['country']).size()\n",
    "print(\"counts\",dfcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nunique() method returns the number of unique values for each column.\n",
    "\n",
    "By specifying the column axis (axis='columns'), the nunique() method searches column-wise and returns the number of unique values for each row.\n",
    "\n",
    "Syntax\n",
    "dataframe.nunique(axis, dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfconv.nunique('columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff types is the problem\n",
    "dfconv.apply(lambda x: x.value_counts()).T.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df1.groupby(['country', 'stateProvince', 'scientificName']).size() \n",
    "print(count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = dfconv.groupby(['country', 'stateProvince', 'scientificName']).size() \n",
    "print(count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df['fruit'].value_counts()['apple']\n",
    "\n",
    "print(f\"The number of apples is: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the presence of non-native plant species affect bumblebee populations?\t\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame on 'index', 'columns', and 'values'\n",
    "pivot_df = df.pivot(index='date', columns='category', values='value')\n",
    "print(pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample time series data to monthly frequency\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "monthly_df = df.resample('M').sum()\n",
    "print(monthly_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove duplicate rows based on all columns\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving imported file to workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# File path and name\n",
    "file_path = r\"/workspaces/myfolder/MachineLearning/hmeq.csv\"\n",
    " \n",
    "# Specify the URL of the CSV file\n",
    "url = r\"https://support.sas.com/documentation/onlinedoc/viya/exampledatasets/hmeq.csv\"\n",
    " \n",
    "# Download the and save CSV file to Workbench\n",
    "response = requests.get(url)\n",
    "with open(file_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "    print(f'File downloaded:{file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workbench Python",
   "language": "python",
   "name": "workbench_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
